{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/flo/Data/Uni/NAIST/thesis/code/correct-pose\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd \"..\"\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from data_utils import datasets\n",
    "from evaluation.evaluator import Evaluator\n",
    "from networks.pose_correctors import PoseCorrectorGNNv1\n",
    "from evaluation.analyzation import ExperimentAnalyzer, ModelAnalyzer\n",
    "from data_utils import normalization\n",
    "from data_utils.visualization import PoseVisualizer\n",
    "from training.solver import Solver\n",
    "from data_utils import distorters\n",
    "from evaluation import errors\n",
    "from data_utils import normalization as norm\n",
    "from data_utils import helper as data_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'deterministic_mode': True,\n",
      "    'init_weights_path': None,\n",
      "    'n_repetitions': 1,\n",
      "    'name': 'hands17sc_mlp_finconf5_6',\n",
      "    'normalizer': None,\n",
      "    'target_device': device(type='cuda', index=2),\n",
      "    'train_set': 'HANDS17_DPREN_SubjClust_train',\n",
      "    'train_set_size': 757796,\n",
      "    'use_preset': False,\n",
      "    'val_set': 'HANDS17_DPREN_SubjClust_val',\n",
      "    'val_set_size': 198791}\n",
      "\n",
      "{   'batch_size': 2048,\n",
      "    'interest_keys': [   ('distorter_args', 'strength_loc'),\n",
      "                         ('distorter_args', 'strength_scale')],\n",
      "    'log_frequency': 10,\n",
      "    'log_grad': False,\n",
      "    'log_loss': True,\n",
      "    'num_epochs': 70,\n",
      "    'show_plots': False,\n",
      "    'val_example_indices': [0],\n",
      "    'val_example_subset': 'DEFAULT',\n",
      "    'verbose': False}\n",
      "\n",
      "{   'augmenters': [],\n",
      "    'distorter': <class 'data_utils.distorters.KNNPredefinedDistorter'>,\n",
      "    'distorter_args': {   'confusion_prob': 0.0,\n",
      "                          'device': device(type='cuda', index=2),\n",
      "                          'knn_name': 'HANDS17_DPREN_SubjClust_train_labels_shift_16',\n",
      "                          'layer_probs': [0.95, 0.05],\n",
      "                          'layer_radii': [0.0, 1.0],\n",
      "                          'max_k': 4,\n",
      "                          'source_name': 'HANDS17_DPREN_SubjClust_train',\n",
      "                          'stds': [0.05, 0.1],\n",
      "                          'strength_alpha': -4.0,\n",
      "                          'strength_loc': 0.9,\n",
      "                          'strength_scale': 0.02},\n",
      "    'eval_space': 'default',\n",
      "    'loss_function': CombinedMetricLoss(),\n",
      "    'loss_space': 'default',\n",
      "    'model': <class 'networks.pose_correctors.AdditivePoseCorrectorMLP'>,\n",
      "    'model_args': {   'activation_func': LeakyReLU(negative_slope=0.1),\n",
      "                      'batchnorm': False,\n",
      "                      'dropout': 0.2,\n",
      "                      'hidden_dims': [1024, 1024, 1024]},\n",
      "    'optimizer': <class 'torch.optim.adam.Adam'>,\n",
      "    'optimizer_args': {   'betas': [0.9, 0.999],\n",
      "                          'eps': 1e-08,\n",
      "                          'lr': 5e-05,\n",
      "                          'weight_decay': 0.0002},\n",
      "    'scheduler': <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>,\n",
      "    'scheduler_args': {   'factor': 0.5,\n",
      "                          'mode': 'min',\n",
      "                          'patience': 8,\n",
      "                          'verbose': True},\n",
      "    'scheduler_requires_metric': True}\n",
      "Error functions:[<function coordinate_difference at 0x7f5e41279488>, <function bone_length_error at 0x7f5e41279598>] \n",
      "Weights: [0.8, 0.2]\tMetric modes: ['absolute', 'absolute']\n"
     ]
    }
   ],
   "source": [
    "# Load experiment results\n",
    "experiment_name = 'hands17sc_mlp_finconf5_6'\n",
    "analyzer = ExperimentAnalyzer(experiment_name)\n",
    "analyzer.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default:\n",
      "\n",
      "\tDEFAULT:\n",
      "\t\t\t\tValue\tIndices\tVariance\n",
      "\t\tcoord_diff       4.8447\t(3, 0)\t4.36e-03\n",
      "\t\tdistance         9.8084\t(3, 0)\t1.49e-02\n",
      "\t\tbone_length      3.4102\t(7, 0)\t8.32e-03\n",
      "\t\tproportion       0.1273\t(4, 0)\t2.38e-06\n",
      "\n",
      "\tBest on average:\n",
      "\t\t\t\tValue\tIndices\n",
      "\t\tcoord_diff       4.8447\t(3, 0)\n",
      "\t\tdistance         9.8084\t(3, 0)\n",
      "\t\tbone_length      3.4102\t(7, 0)\n",
      "\t\tproportion       0.1273\t(4, 0)\n",
      "\n",
      "\tHyperparameters:\n",
      "\t\tParams of interest: [('distorter_args', 'strength_loc'), ('distorter_args', 'strength_scale')]\n",
      "\t\tSession 3: \t0.94,  0.02,  \n",
      "\t\tSession 4: \t0.94,  0.03,  \n",
      "\t\tSession 7: \t0.96,  0.03,  \n"
     ]
    }
   ],
   "source": [
    "analyzer.print_best_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results of specific model\n",
    "idx = (14, 0)\n",
    "print(analyzer.results[idx[0]][idx[1]])\n",
    "print(analyzer.hyperparams[idx[0]]['optimizer_args']['weight_decay'])\n",
    "print(analyzer.hyperparams[idx[0]]['model_args']['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyzer.plot_average_model_performances('distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.print_params_ranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_name = ('distorter_args', 'strength_loc')\n",
    "#print(analyzer.param_value_index_mapping[param_name])\n",
    "print([str(val) for val in analyzer.param_value_index_mapping[param_name][0]])\n",
    "analyzer.plot_average_parameter_performances(param_name, 'proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select one\n",
    "idx = (0, 0)\n",
    "\n",
    "dataset = datasets.PairedPoseDataset('HANDS17_DPREN_SubjClust_val', distorters.NoDistorter(), use_preset=True, device='cuda:0')\n",
    "\n",
    "model_dir = os.path.join(analyzer.experiment_dir, str(idx[0]), str(idx[1]))\n",
    "model_analyzer = ModelAnalyzer(model_dir, analyzer.hyperparams[idx[0]], dataset, analyzer.config)\n",
    "model_analyzer.print_hyperparameters()\n",
    "print(model_analyzer.hyperparams['loss_function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model_analyzer.log['best_epoch'])\n",
    "print(min(model_analyzer.log['val']['DEFAULT']['distance']))\n",
    "model_analyzer.plot_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer.evaluate_dataset()\n",
    "#model_analyzer.evaluate_dataset(mode='max')\n",
    "#model_analyzer.evaluate_model(mode='max')\n",
    "model_analyzer.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer.save_dataset_errors('HANDS17ssp_DPREN_val_noshift_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer.compare_success_rate_to(model_analyzer.dataset_errors, 'default', 'DEFAULT', 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare evaluation results\n",
    "model_analyzer.compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wild collection of old evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_errors = []\n",
    "for k in range(5, 10):\n",
    "    print(k)\n",
    "    model_analyzer.model.n_iter = k\n",
    "    model_analyzer.evaluate_model()\n",
    "    model_analyzer.compare_results()\n",
    "    model_errors.append(model_analyzer.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_distance_errors = []\n",
    "average_bone_length_errors = []\n",
    "average_proportion_errors = []\n",
    "\n",
    "for k_errors in model_errors:\n",
    "    k_average_errors = Evaluator.means_per_metric(k_errors['default'])\n",
    "    average_distance_errors.append(k_average_errors['DEFAULT']['distance'])\n",
    "    average_bone_length_errors.append(k_average_errors['DEFAULT']['bone_length'])\n",
    "    average_proportion_errors.append(k_average_errors['DEFAULT']['proportion'])\n",
    "    \n",
    "average_distance_errors = torch.stack(average_distance_errors).cpu().numpy()\n",
    "average_bone_length_errors = torch.stack(average_bone_length_errors).cpu().numpy()\n",
    "average_proportion_errors = torch.stack(average_proportion_errors).cpu().numpy()\n",
    "\n",
    "average_distance_errors = average_distance_errors / np.max(average_distance_errors)\n",
    "average_bone_length_errors = average_bone_length_errors / np.max(average_bone_length_errors)\n",
    "average_proportion_errors = average_proportion_errors / np.max(average_proportion_errors)\n",
    "\n",
    "plt.plot(average_distance_errors, label='Distance')\n",
    "plt.plot(average_bone_length_errors, label='Bone length')\n",
    "plt.plot(average_proportion_errors, label='Proportion')\n",
    "plt.xlabel('Message passing iterations')\n",
    "plt.ylabel('Standardized average error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select different model\n",
    "experiment_name_2 = 'hands17ssp_mlp_knndist_nopre_3'\n",
    "analyzer_2 = ExperimentAnalyzer(experiment_name_2)\n",
    "idx_2 = (0, 1)\n",
    "#dataset_2 = datasets.NormalizedPairedPoseDataset('HANDS17_DPREN_ShapeSplit_val', distorters.NoDistorter(), norm.Shifter, True, device='cuda:0')\n",
    "dataset_2 = dataset\n",
    "model_dir_2 = os.path.join(analyzer_2.experiment_dir, str(idx_2[0]), str(idx_2[1]))\n",
    "model_analyzer_2 = ModelAnalyzer(model_dir_2, analyzer_2.hyperparams[idx_2[0]], dataset_2, analyzer_2.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = analyzer_2.results[idx_2[0]][idx_2[1]]\n",
    "#results_2['default'] = results_2['original']\n",
    "#results_2['original'] = results_2['default']\n",
    "model_analyzer_2.evaluate_model()\n",
    "model_analyzer_2.evaluate_dataset('HANDS17ssp_DPREN_val_noshift')\n",
    "#model_analyzer_2.compare_results()\n",
    "\n",
    "#model_analyzer.compare_success_rate_to(model_analyzer_2.errors, 'default', 'DEFAULT', 'distance')\n",
    "#model_analyzer_2.compare_success_rate_to(model_analyzer_2.dataset_errors, 'default', 'DEFAULT', 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator.print_comparison({'default': Evaluator.means_per_metric(model_analyzer.errors['default'])}, {'default': Evaluator.means_per_metric(model_analyzer_2.errors['default'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analyzer_2.plot_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = dataset[:].poses\n",
    "\n",
    "pose_center_shift_error_before = torch.norm(labels.mean(dim=1).cpu() - poses.mean(dim=1).cpu(), dim=1)\n",
    "pose_center_shift_errors_1 = torch.norm(labels.mean(dim=1).cpu() - model_analyzer.predictions.mean(dim=1), dim=1)\n",
    "pose_center_shift_errors_2 = torch.norm(labels.mean(dim=1).cpu() - model_analyzer_2.predictions.mean(dim=1), dim=1)\n",
    "print('Mean pose center shift errors: before: {},\\t 1: {},\\t2: {}'.format(pose_center_shift_error_before.mean(), pose_center_shift_errors_1.mean(), pose_center_shift_errors_2.mean()))\n",
    "print(pose_center_shift_errors_1.mean() - pose_center_shift_errors_2.mean())\n",
    "\n",
    "wrong_shifts_1 = pose_center_shift_errors_1[pose_center_shift_errors_1 > pose_center_shift_error_before]\n",
    "wrong_shifts_2 = pose_center_shift_errors_2[pose_center_shift_errors_2 > pose_center_shift_error_before]\n",
    "\n",
    "print('Wrong shifts 1:', len(wrong_shifts_1), wrong_shifts_1.mean(), wrong_shifts_1.max())\n",
    "print('Wrong shifts 2:', len(wrong_shifts_2), wrong_shifts_2.mean(), wrong_shifts_2.max())\n",
    "\n",
    "good_shifts_1 = pose_center_shift_errors_1[pose_center_shift_errors_1 < pose_center_shift_error_before]\n",
    "good_shifts_2 = pose_center_shift_errors_2[pose_center_shift_errors_2 < pose_center_shift_error_before]\n",
    "\n",
    "print('Good shifts 1:', len(good_shifts_1), good_shifts_1.mean(), good_shifts_1.min())\n",
    "print('Good shifts 2:', len(good_shifts_2), good_shifts_2.mean(), good_shifts_2.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_errors_1 = model_analyzer.errors['default']['DEFAULT']['distance']\n",
    "distance_errors_2 = model_analyzer_2.errors['default']['DEFAULT']['distance']\n",
    "improvements =  distance_errors_1 - distance_errors_2\n",
    "best_improvement_idx = torch.argmin(improvements)\n",
    "\n",
    "pred_1 = model_analyzer.predictions[best_improvement_idx]\n",
    "pred_2 = model_analyzer_2.predictions[best_improvement_idx]\n",
    "label = labels[best_improvement_idx].cpu()\n",
    "input_pose = dataset[best_improvement_idx].poses.squeeze().cpu()\n",
    "\n",
    "PoseVisualizer.n_tuple(torch.stack([input_pose, pred_1, pred_2, label]), labels=['Input', 'No shift', 'Shift', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improv_mask = improvements < 0.0\n",
    "worse_mask = improvements > 0.0\n",
    "\n",
    "print(improv_mask.sum())\n",
    "print(worse_mask.sum())\n",
    "print(improvements[improv_mask].mean(), improvements[improv_mask].min())\n",
    "print(improvements[worse_mask].mean(), improvements[worse_mask].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset[:10]\n",
    "#model_output = model_analyzer.model(batch.poses)\n",
    "#print(model_output[1])\n",
    "print(model_analyzer.model.normalizer(batch.poses.reshape(-1, 63)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_diff = model_analyzer.errors['original']['DEFAULT']['distance'] - model_analyzer_2.errors['original']['DEFAULT']['distance']\n",
    "print(torch.min(result_diff))\n",
    "print(torch.max(result_diff))\n",
    "plt.hist(result_diff.cpu().numpy(), 100, (-50, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = torch.argmin(result_diff)\n",
    "sample = dataset[best_idx]\n",
    "#pose_1 = normalization.denormalize_poses(model_analyzer.model(sample.poses), sample.normalization_params)\n",
    "#pose_2 = normalization.denormalize_poses(model_analyzer_2.model(sample.poses), sample.normalization_params)\n",
    "pose_1 = model_analyzer.model(sample.poses)\n",
    "pose_2 = model_analyzer_2.model(sample.poses)\n",
    "\n",
    "\n",
    "#PoseVisualizer.triplet(torch.stack((sample.original_poses.cpu(), pose_2.detach().cpu(), sample.original_labels.detach().cpu())))\n",
    "#errors_1 = torch.norm(pose_1 - sample.original_labels, dim=2)\n",
    "#errors_2 = torch.norm(pose_2 - sample.original_labels, dim=2)\n",
    "#print(errors_1 - errors_2)\n",
    "#PoseVisualizer.triplet(torch.stack((pose_1.detach().cpu(), pose_2.detach().cpu(), sample.original_labels.detach().cpu())))\n",
    "PoseVisualizer.triplet(torch.stack((sample.poses.cpu(), pose_2.detach().cpu(), sample.labels.cpu())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find extreme cases\n",
    "val_data = datasets.NormalizedPairedPoseDataset(analyzer.config['experiment']['val_set'], distorters.NoDistorter(), True, device='cuda:0')\n",
    "model = hyperparams['model'](hyperparams['model_args'])\n",
    "weights = torch.load(os.path.join(analyzer.experiment_dir, str(idx[0]), str(idx[1]), 'weights.pt'), map_location='cpu')\n",
    "model.load_state_dict(weights)\n",
    "model.cuda()\n",
    "\n",
    "val_data.select_subset('3DCNN')\n",
    "batch = val_data[:3000]\n",
    "predictions = model.test(batch.poses).detach()\n",
    "\n",
    "# Just for large offsets between input and prediction\n",
    "#eval_batch = datasets.PoseCorrectionBatch(predictions, batch.poses)\n",
    "#diffs = Evaluator.to_batch(eval_batch)\n",
    "\n",
    "# For the \"best\" sample:\n",
    "errors_before = Evaluator.to_batch(batch)\n",
    "eval_batch = datasets.PoseCorrectionBatch(predictions, batch.labels)\n",
    "errors_after = Evaluator.to_batch(eval_batch)\n",
    "diff = errors_before['distance'] - errors_after['distance']\n",
    "\n",
    "#most_significant_idx = torch.argmax(diffs['distance'])\n",
    "most_significant_idx = torch.argmax(diff)\n",
    "pose = batch.poses[most_significant_idx].reshape(1, 21, 3)\n",
    "label = batch.labels[most_significant_idx].reshape(1, 21, 3)\n",
    "prediction = predictions[most_significant_idx].reshape(1, 21, 3)\n",
    "\n",
    "\n",
    "PoseVisualizer.triplet(pose, prediction, label, [0, 1])\n",
    "#PoseVisualizer.triplet(denorm_poses['3DCNN'], denorm_pred, denorm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate model\n",
    "val_data = datasets.NormalizedPairedPoseDataset(analyzer.config['experiment']['val_set'], distorters.NoDistorter(), True, device='cuda:0')\n",
    "model = hyperparams['model'](hyperparams['model_args'])\n",
    "weights = torch.load(os.path.join(analyzer.experiment_dir, str(idx[0]), str(idx[1]), 'weights.pt'), map_location='cpu')\n",
    "model.load_state_dict(weights)\n",
    "model.cuda()\n",
    "\n",
    "data_loader = datasets.DataLoader(val_data, 100)\n",
    "eval_results = Evaluator.to_model(data_loader, model, space='original')\n",
    "mean_results = Evaluator.means_per_metric(eval_results)\n",
    "Evaluator.print_results({'original': mean_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2281\n",
    "pose = batch.poses[idx].reshape(1, 21, 3)\n",
    "label = batch.labels[idx].reshape(1, 21, 3)\n",
    "prediction = predictions[idx].reshape(1, 21, 3)\n",
    "PoseVisualizer.triplet(pose, prediction, label, [0, 1, 2, 3, 4], True)\n",
    "\n",
    "pose = batch.original_poses[idx].reshape(1, 21, 3)\n",
    "label = batch.original_labels[idx].reshape(1, 21, 3)\n",
    "prediction = normalization.denormalize_poses(predictions, batch.normalization_params)[idx].reshape(1, 21, 3)\n",
    "PoseVisualizer.triplet(pose, prediction, label, [0, 1, 2, 3, 4], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PoseVisualizer.visualize_evolution(example_data.poses[i].reshape(1, 21, 3), example_predictions[:, 0], example_data.labels[i].reshape(1, 21, 3), [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions of GNN and MLP\n",
    "val_set = datasets.PairedPoseDataset('HANDS17_DPREN_ShapeSplitPruned_val', use_preset=True, device='cuda:0',)\n",
    "\n",
    "gnn_experiment_dir = 'results/hands17ssp_gnn1_knndist_nopre_1'\n",
    "gnn_idx = (0, 0)\n",
    "mlp_experiment_dir = 'results/hands17ssp_mlp_knndist_nopre_3'\n",
    "mlp_idx = (0, 1)\n",
    "\n",
    "gnn_training_config = torch.load(os.path.join(gnn_experiment_dir, 'config.pt'))\n",
    "mlp_training_config = torch.load(os.path.join(mlp_experiment_dir, 'config.pt'))\n",
    "\n",
    "gnn_session_dir = os.path.join(gnn_experiment_dir, str(gnn_idx[0]))\n",
    "mlp_session_dir = os.path.join(mlp_experiment_dir, str(mlp_idx[0]))\n",
    "gnn_hyperparams = torch.load(os.path.join(gnn_session_dir, 'params.pt'))\n",
    "mlp_hyperparams = torch.load(os.path.join(mlp_session_dir, 'params.pt'))\n",
    "\n",
    "gnn_model_dir = os.path.join(gnn_session_dir, str(gnn_idx[1]))\n",
    "mlp_model_dir = os.path.join(mlp_session_dir, str(mlp_idx[1]))\n",
    "\n",
    "gnn_analyzer = ModelAnalyzer(gnn_model_dir, gnn_hyperparams, val_set, gnn_training_config)\n",
    "mlp_analyzer = ModelAnalyzer(mlp_model_dir, mlp_hyperparams, val_set, mlp_training_config)\n",
    "\n",
    "gnn_analyzer.test_model()\n",
    "mlp_analyzer.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = val_set[:].labels.cpu()\n",
    "poses = val_set[:].poses.cpu()\n",
    "ensemble_predictions = torch.stack([gnn_analyzer.predictions, mlp_analyzer.predictions], dim=1).mean(dim=1)\n",
    "\n",
    "gnn_distance_errors = torch.norm(labels - gnn_analyzer.predictions, dim=2).mean(dim=1)\n",
    "mlp_distance_errors = torch.norm(labels - mlp_analyzer.predictions, dim=2).mean(dim=1)\n",
    "ensemble_distance_errors = torch.norm(labels - ensemble_predictions, dim=2).mean(dim=1)\n",
    "\n",
    "gnn_center_shift_vecs = labels.mean(dim=1) - gnn_analyzer.predictions.mean(dim=1)\n",
    "mlp_center_shift_vecs = labels.mean(dim=1) - mlp_analyzer.predictions.mean(dim=1)\n",
    "\n",
    "gnn_center_shifts = torch.norm(gnn_center_shift_vecs, dim=1)\n",
    "mlp_center_shifts = torch.norm(mlp_center_shift_vecs, dim=1)\n",
    "\n",
    "initial_distance_errors = torch.norm(labels - poses, dim=2).mean(dim=1)\n",
    "gnn_total_improv = gnn_distance_errors - initial_distance_errors\n",
    "mlp_total_improv = mlp_distance_errors - initial_distance_errors\n",
    "\n",
    "print('Initial distance error: {:.2f}'.format(initial_distance_errors.mean()))\n",
    "print('GNN successfully improved the input in {:.1%} of cases'.format((gnn_total_improv < 0).sum().type(torch.float32) / len(val_set)))\n",
    "print('MLP successfully improved the input in {:.1%} of cases'.format((mlp_total_improv < 0).sum().type(torch.float32) / len(val_set)))\n",
    "\n",
    "print('Average distance error GNN: {:.2f}'.format(gnn_distance_errors.mean()))\n",
    "print('Average distance error MLP: {:.2f}'.format(mlp_distance_errors.mean()))\n",
    "print('Pose center shift distances GNN: {:.2f}'.format(gnn_center_shifts.mean()))\n",
    "print('Pose center shift distances MLP: {:.2f}'.format(mlp_center_shifts.mean()))\n",
    "\n",
    "gnn_pred_shift_component = gnn_analyzer.predictions.mean(dim=1) - poses.mean(dim=1)\n",
    "mlp_pred_shift_component = mlp_analyzer.predictions.mean(dim=1) - poses.mean(dim=1)\n",
    "ensemble_pred_shift_component = ensemble_predictions.mean(dim=1) - poses.mean(dim=1)\n",
    "\n",
    "gnn_shiftonly_preds = poses + gnn_pred_shift_component.view(-1, 1, 3)\n",
    "mlp_shiftonly_preds = poses + mlp_pred_shift_component.view(-1, 1, 3)\n",
    "ensemble_shiftonly_preds = poses + ensemble_pred_shift_component.view(-1, 1, 3)\n",
    "\n",
    "gnn_shiftonly_distance_errors = torch.norm(labels - gnn_shiftonly_preds, dim=2).mean(dim=1)\n",
    "mlp_shiftonly_distance_errors = torch.norm(labels - mlp_shiftonly_preds, dim=2).mean(dim=1)\n",
    "ensemble_shiftonly_distance_errors = torch.norm(labels - ensemble_shiftonly_preds, dim=2).mean(dim=1)\n",
    "\n",
    "print('Average distance error for GNN when only applying the shifts: {:.2f}'.format(gnn_shiftonly_distance_errors.mean()))\n",
    "print('Average distance error for MLP when only applying the shifts: {:.2f}'.format(mlp_shiftonly_distance_errors.mean()))\n",
    "\n",
    "ensemble_errors = Evaluator.means_per_metric({'default': Evaluator.to_batch(datasets.PoseCorrectionBatch(ensemble_predictions, labels))})\n",
    "print('Average distance error ensemble: {:.3f}'.format(ensemble_errors['default']['distance']))\n",
    "print('Average bone length error ensemble: {:.3f}'.format(ensemble_errors['default']['bone_length']))\n",
    "print('Average proportion error ensemble: {:.3f}'.format(ensemble_errors['default']['proportion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Variance GNN: {:.2f}'.format(torch.var(gnn_distance_errors)))\n",
    "print('Variance MLP: {:.2f}'.format(torch.var(mlp_distance_errors)))\n",
    "\n",
    "gnn_improv = gnn_distance_errors - mlp_distance_errors\n",
    "gnn_center_improv = gnn_center_shifts - mlp_center_shifts\n",
    "\n",
    "print('GNN is better in {:.1%} of cases'.format((gnn_improv < 0).sum().type(torch.float32) / len(val_set)))\n",
    "print('Max improvement: {:.2f}'.format(gnn_improv.min()))\n",
    "print('Max worsening: {:.2f}'.format(gnn_improv.max()))\n",
    "print('Max center shift improvement: {:.2f}'.format(gnn_center_improv.min()))\n",
    "print('Max center shift worsening: {:.2f}'.format(gnn_center_improv.max()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "abs_diff = gnn_improv.abs()\n",
    "abs_center_diff = gnn_center_improv.abs()\n",
    "\n",
    "thresholds = np.linspace(0.0, abs_diff.max().cpu().numpy(), 100)\n",
    "\n",
    "n_lower = [(abs_diff < threshold).sum().type(torch.float32).cpu().numpy() / len(val_set) * 100 for threshold in thresholds]\n",
    "n_center_lower = [(abs_center_diff < threshold).sum().type(torch.float32).cpu().numpy() / len(val_set) * 100 for threshold in thresholds]\n",
    "\n",
    "axes[0].plot(thresholds, n_lower, label='Distance error')\n",
    "axes[0].plot(thresholds, n_center_lower, label='Center shift')\n",
    "axes[0].set_xlabel('Absolute difference between MLP and GNN')\n",
    "axes[0].set_ylabel('Percentage of samples with lower differences')\n",
    "axes[0].legend()\n",
    "\n",
    "improv_mask = gnn_improv < 0.0\n",
    "n_lower_better = [(gnn_improv[improv_mask] > -threshold).sum().type(torch.float32).cpu().numpy() / improv_mask.sum().item() * 100 for threshold in thresholds]\n",
    "n_lower_worse = [(gnn_improv[~improv_mask] < threshold).sum().type(torch.float32).cpu().numpy() / (~improv_mask).sum().item() * 100 for threshold in thresholds]\n",
    "\n",
    "axes[1].plot(thresholds, n_lower_better, label='GNN better than MLP')\n",
    "axes[1].plot(thresholds, n_lower_worse, label='GNN worse than MLP')\n",
    "axes[1].set_xlabel('Absolute difference between MLP and GNN')\n",
    "axes[1].set_ylabel('Percentage of samples with lower differences')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 15), squeeze=False, sharey=True)\n",
    "\n",
    "gnn_after_shift_improv = gnn_distance_errors - gnn_shiftonly_distance_errors\n",
    "mlp_after_shift_improv = mlp_distance_errors - mlp_shiftonly_distance_errors\n",
    "ensemble_after_shift_improv = ensemble_distance_errors - ensemble_shiftonly_distance_errors\n",
    "\n",
    "gnn_pred_shift_distances = gnn_pred_shift_component.norm(dim=1).cpu().numpy()\n",
    "mlp_pred_shift_distances = mlp_pred_shift_component.norm(dim=1).cpu().numpy()\n",
    "ensemble_pred_shift_distances = ensemble_pred_shift_component.norm(dim=1).numpy()\n",
    "\n",
    "axes[0, 0].scatter(gnn_pred_shift_distances, gnn_after_shift_improv.cpu().numpy(), s=1, alpha=0.1)\n",
    "axes[0, 0].set_ylabel('Improvement after shift')\n",
    "axes[0, 0].set_title('GNN')\n",
    "axes[0, 1].scatter(mlp_pred_shift_distances, mlp_after_shift_improv.cpu().numpy(), s=1, alpha=0.1)\n",
    "axes[0, 1].set_title('MLP')\n",
    "axes[0, 0].axhline(y=0, color='grey', alpha=0.5)\n",
    "axes[0, 1].axhline(y=0, color='grey', alpha=0.5)\n",
    "fig.text(0.5, 0.00, 'Predicted shift distance', ha='center')\n",
    "\n",
    "def slice_variance(x, y, lower_threshold, upper_threshold):\n",
    "    below_upper_mask = x < upper_threshold\n",
    "    above_lower_mask = x > lower_threshold\n",
    "    between_mask = below_upper_mask * above_lower_mask\n",
    "    subset = y[between_mask]\n",
    "    return np.nan_to_num(subset.var(), 0.0)\n",
    "\n",
    "thresholds = np.linspace(0.0, max(gnn_pred_shift_distances.max(), mlp_pred_shift_distances.max()), 40)\n",
    "gnn_improv_variances = [slice_variance(gnn_pred_shift_distances, gnn_after_shift_improv.cpu().numpy(), t1, t2) for t1, t2 in zip(thresholds[:-1], thresholds[1:])]\n",
    "mlp_improv_variances = [slice_variance(mlp_pred_shift_distances, mlp_after_shift_improv.cpu().numpy(), t1, t2) for t1, t2 in zip(thresholds[:-1], thresholds[1:])]\n",
    "\n",
    "ax0_twin = axes[0, 0].twinx()\n",
    "ax0_twin.plot(thresholds[:-1], gnn_improv_variances, color='orange')\n",
    "ax0_twin.set_ylim((0.0, 4.0))\n",
    "\n",
    "ax1_twin = axes[0, 1].twinx()\n",
    "ax1_twin.plot(thresholds[:-1], mlp_improv_variances, color='orange')\n",
    "ax1_twin.set_ylabel('Variance')\n",
    "ax1_twin.set_ylim((0.0, 4.0))\n",
    "\n",
    "ensemble_uncertainties = torch.norm(gnn_analyzer.predictions - mlp_analyzer.predictions, dim=2).mean(dim=1)\n",
    "topk_uncertainties, topk_uncertain_indices = torch.topk(ensemble_uncertainties, k=100, largest=True)\n",
    "print(topk_uncertainties.min())\n",
    "\n",
    "axes[1, 0].scatter(ensemble_pred_shift_distances, ensemble_after_shift_improv.numpy(), s=1, alpha=0.1)\n",
    "axes[1, 0].scatter(ensemble_pred_shift_distances[topk_uncertain_indices], ensemble_after_shift_improv[topk_uncertain_indices].numpy(), s=2, alpha=0.5, color='red')\n",
    "axes[1, 0].axhline(y=0, color='grey', alpha=0.5)\n",
    "axes[1, 1].scatter(gnn_pred_shift_distances, gnn_total_improv.cpu().numpy(), s=1, alpha=0.1)\n",
    "axes[1, 1].scatter(gnn_pred_shift_distances[topk_uncertain_indices], gnn_total_improv[topk_uncertain_indices].cpu().numpy(), s=2, alpha=0.5, color='red')\n",
    "axes[1, 1].axhline(y=0, color='grey', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Select subset where the predicted shift is low --> visualize the best and worst corrections from GNN vs MLP\n",
    "\n",
    "pred_shift_distance_threshold = 1.0\n",
    "mask = torch.from_numpy(gnn_pred_shift_distances < pred_shift_distance_threshold)\n",
    "print(len(torch.nonzero(mask)))\n",
    "\n",
    "print('Subset mean distance error GNN:', gnn_distance_errors[mask].mean())\n",
    "print('Subset mean distance error MLP:', mlp_distance_errors[mask].mean())\n",
    "\n",
    "k = 6\n",
    "topk_improvements = torch.topk(gnn_improv[mask], k=k, largest=False)\n",
    "topk_worsenings = torch.topk(gnn_improv[mask], k=k, largest=True)\n",
    "\n",
    "print('Top k improvements')\n",
    "print('Improvement\\tGNN Error\\tMLP Error\\tCenter Shift Improvement')\n",
    "for idx in topk_improvements.indices:\n",
    "    print('{:.2f}\\t\\t{:.2f}\\t\\t{:.2f}\\t\\t{:.2f}'.format(gnn_improv[mask][idx], gnn_distance_errors[mask][idx], mlp_distance_errors[mask][idx], gnn_center_improv[mask][idx]))\n",
    "\n",
    "print('\\nTop k worsenings')\n",
    "print('Improvement\\tGNN Error\\tMLP Error\\tCenter Shift Improvement')\n",
    "for idx in topk_worsenings.indices:\n",
    "    print('{:.2f}\\t\\t{:.2f}\\t\\t{:.2f}\\t\\t{:.2f}'.format(gnn_improv[mask][idx], gnn_distance_errors[mask][idx], mlp_distance_errors[mask][idx], gnn_center_improv[mask][idx]))\n",
    "    \n",
    "batch = val_set[mask]\n",
    "gnn_triplet_batch = torch.stack([batch.poses[topk_improvements.indices].cpu(),\n",
    "                                 gnn_analyzer.predictions[mask][topk_improvements.indices], \n",
    "                                 batch.labels[topk_improvements.indices].cpu()], dim=1)\n",
    "mlp_triplet_batch = torch.stack([batch.poses[topk_improvements.indices].cpu(),\n",
    "                                 mlp_analyzer.predictions[mask][topk_improvements.indices], \n",
    "                                 batch.labels[topk_improvements.indices].cpu()], dim=1)\n",
    "\n",
    "PoseVisualizer.triplet_batch(gnn_triplet_batch)\n",
    "PoseVisualizer.triplet_batch(mlp_triplet_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_gnn_errors, topk_gnn_error_indices = torch.topk(gnn_total_improv[mask], k=k, largest=False)\n",
    "gnn_triplet_batch = torch.stack([batch.poses[topk_gnn_error_indices].cpu(),\n",
    "                                 gnn_analyzer.predictions[mask][topk_gnn_error_indices], \n",
    "                                 batch.labels[topk_gnn_error_indices].cpu()], dim=1)\n",
    "PoseVisualizer.triplet_batch(gnn_triplet_batch, visible_fingers=[0, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gnn_corrected_poses = gnn_analyzer.predictions[:100]\n",
    "mlp_corrected_poses = mlp_analyzer.predictions[:100]\n",
    "\n",
    "print('GNN corrected once: {:.2f}'.format(torch.norm(gnn_corrected_poses - labels[:100], dim=2).mean()))\n",
    "print('MLP corrected once: {:.2f}'.format(torch.norm(mlp_corrected_poses - labels[:100], dim=2).mean()))\n",
    "\n",
    "gnn_double_corrected = gnn_analyzer.model.test(gnn_corrected_poses.cuda())\n",
    "mlp_double_corrected = mlp_analyzer.model.test(mlp_corrected_poses.cuda())\n",
    "\n",
    "print('GNN corrected once: {:.2f}'.format(torch.norm(gnn_double_corrected.cpu() - labels[:100], dim=2).mean()))\n",
    "print('MLP corrected once: {:.2f}'.format(torch.norm(mlp_double_corrected.cpu() - labels[:100], dim=2).mean()))\n",
    "\n",
    "PoseVisualizer.triplet_batch(torch.stack([poses[:9], gnn_corrected_poses[:9], gnn_double_corrected[:9].cpu().detach()], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_analyzer.predictions, 'data/HANDS17_CorrctMLP_SSP_train_poses.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
